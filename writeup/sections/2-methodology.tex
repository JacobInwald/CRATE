\chapter{Methodology}

In this chapter we will discuss our approach to collecting and interrogating data. 
We aim to investigate whether different hardening options interact (\emph{RQ1}) and whether those interactions lead to security being lost or gained (\emph{RQ2}). 
As measuring the absolute security of a piece of software is a complex and multi-faceted process \citep{??}, we have limited the scope of this study to the 5 flags specified in Section~\ref{sec:background}. 
In the further sections, we outline the structure of our tests, the data sources used, the approach to curating our dataset and how we will use the curated data. 


\section{Test Structure \& Data Sources}

Past literature that discusses the security of hardening options will often utilize known exploit-code pairs to test this \citep{??}. 
Inline with this practice, we use this method to test the security of combinations of different hardening options (\emph{RQ1}, \emph{RQ2}). 
We use 5 exploit-code pairs for each of the chosen options, to provide relevant tests for each specific option, allowing us to investigate the effect of combinations afterwards.
To sandbox and ensure reproducibility of the exploit-code pairs, Docker is used \citep{??}. 
All tests are run on x86\_64 Linux, with hardening options that are on by default, such as ASLR, left on to simulate a normal level of security in the system. 

We source our exploits from exploit.db \citep{??} and 0day.today \citep{??}; websites that catalog and store thousands of known exploit-code pairs. 
While these websites share the exploit, they do not necessarily have the related source code, as some exploits are on closed-source software. 
Therefore, we discard closed-source examples, and focus on open-source software which is available on version control platforms such as GitHub \citep{??} and GitLab \citep{??}. 
We further discard examples that do not work on our selected test platform (x86\_64 Linux). 


\section{Data Curation}

At the time of writing, there exists no standard collection of exploit-code pairings which can be easily set up and tested. 
There exist datasets of insecure code such as SARD \citep{??} and SEI CERT code snippets \citep{??}, however the aims of these datasets do not align with the aims of this study. 
SARD is aimed at trained static analysis tools, so the code snippets provided do not have corresponding exploits, and are often taken out of context of the larger program. 
SEI CERT is aimed at demonstrating insecure coding practices, so the code snippets also do not have corresponding exploits. 
Therefore, we have curated a dataset of exploit-code pairings which fit with the aims of the study. 

We use the data sources described above to collect data from. 
First, all exploit-code pairings that are not compatible with our test hardware and are closed-source are discarded. 
Next, for each option, we consider the relevant CWE/s and CVEs that are addressed by that option. 
We then choose exploit-code pairings that are linked to the chosen CVE/CWE; prioritizing pairings that have more available exploits and are more compatible with the test hardware. 
Where possible, we further supplement the data with self-authored exploits. 
Finally, we create Docker containers for each pairing to ensure reproducibility of results. 
To assess the security impact of combination options, we can then compare the effectiveness of the exploits between combinations of flags. 

